<html>
<body style="font-family: arial">
  
<h1 style="text-align: center;">Hallucinations in Healthcare AI</h1>

<p>MA, TR, IY</p>
<p><strong>Corresponding Author:</strong> <a href = "mailto:maahmad@uw.edu">maahmad@uw.edu</a></p>
<h2>Introduction</h2>
<h2>Definitions</h2>
<p></p>
<h2>How to cite</h2>
<p></p>
<h2>References</h2>
<div class="csl-bib-body" style="line-height: 1.35; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Agrawal, Ayush, Lester Mackey, and Adam Tauman Kalai. 2023. “Do Language Models Know When They’re Hallucinating References?” arXiv. <a href="https://doi.org/10.48550/arXiv.2305.18248">https://doi.org/10.48550/arXiv.2305.18248</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2305.18248&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Do%20Language%20Models%20Know%20When%20They're%20Hallucinating%20References%3F&amp;rft.description=Current%20state-of-the-art%20language%20models%20(LMs)%20are%20notorious%20for%20generating%20text%20with%20%22hallucinations%2C%22%20a%20primary%20example%20being%20book%20and%20paper%20references%20that%20lack%20any%20solid%20basis%20in%20their%20training%20data.%20However%2C%20we%20find%20that%20many%20of%20these%20fabrications%20can%20be%20identified%20using%20the%20same%20LM%2C%20using%20only%20black-box%20queries%20without%20consulting%20any%20external%20resources.%20Consistency%20checks%20done%20with%20direct%20queries%20about%20whether%20the%20generated%20reference%20title%20is%20real%20(inspired%20by%20Kadavath%20et%20al.%202022%2C%20Lin%20et%20al.%202022%2C%20Manakul%20et%20al.%202023)%20are%20compared%20to%20consistency%20checks%20with%20indirect%20queries%20which%20ask%20for%20ancillary%20details%20such%20as%20the%20authors%20of%20the%20work.%20These%20consistency%20checks%20are%20found%20to%20be%20partially%20reliable%20indicators%20of%20whether%20or%20not%20the%20reference%20is%20a%20hallucination.%20In%20particular%2C%20we%20find%20that%20LMs%20in%20the%20GPT-series%20will%20hallucinate%20differing%20authors%20of%20hallucinated%20references%20when%20queried%20in%20independent%20sessions%2C%20while%20it%20will%20consistently%20identify%20authors%20of%20real%20references.%20This%20suggests%20that%20the%20hallucination%20may%20be%20more%20a%20result%20of%20generation%20techniques%20than%20the%20underlying%20representation.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2305.18248&amp;rft.aufirst=Ayush&amp;rft.aulast=Agrawal&amp;rft.au=Ayush%20Agrawal&amp;rft.au=Lester%20Mackey&amp;rft.au=Adam%20Tauman%20Kalai&amp;rft.date=2023-05-29"></span>
  <div class="csl-entry">Alkaissi, Hussam, and Samy I McFarlane. n.d. “Artificial Hallucinations in ChatGPT: Implications in Scientific Writing.” <i>Cureus</i> 15 (2): e35179. <a href="https://doi.org/10.7759/cureus.35179">https://doi.org/10.7759/cureus.35179</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.7759%2Fcureus.35179&amp;rft_id=info%3Apmid%2F36811129&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Artificial%20Hallucinations%20in%20ChatGPT%3A%20Implications%20in%20Scientific%20Writing&amp;rft.jtitle=Cureus&amp;rft.stitle=Cureus&amp;rft.volume=15&amp;rft.issue=2&amp;rft.aufirst=Hussam&amp;rft.aulast=Alkaissi&amp;rft.au=Hussam%20Alkaissi&amp;rft.au=Samy%20I%20McFarlane&amp;rft.pages=e35179&amp;rft.issn=2168-8184"></span>
  <div class="csl-entry">Au Yeung, Joshua, Zeljko Kraljevic, Akish Luintel, Alfred Balston, Esther Idowu, Richard J. Dobson, and James T. Teo. 2023. “AI Chatbots Not yet Ready for Clinical Use.” <i>Frontiers in Digital Health</i> 5. <a href="https://www.frontiersin.org/articles/10.3389/fdgth.2023.1161098">https://www.frontiersin.org/articles/10.3389/fdgth.2023.1161098</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=AI%20chatbots%20not%20yet%20ready%20for%20clinical%20use&amp;rft.jtitle=Frontiers%20in%20Digital%20Health&amp;rft.volume=5&amp;rft.aufirst=Joshua&amp;rft.aulast=Au%20Yeung&amp;rft.au=Joshua%20Au%20Yeung&amp;rft.au=Zeljko%20Kraljevic&amp;rft.au=Akish%20Luintel&amp;rft.au=Alfred%20Balston&amp;rft.au=Esther%20Idowu&amp;rft.au=Richard%20J.%20Dobson&amp;rft.au=James%20T.%20Teo&amp;rft.date=2023&amp;rft.issn=2673-253X"></span>
  <div class="csl-entry">“Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine | NEJM.” n.d. Accessed September 12, 2023. <a href="https://www.nejm.org/doi/pdf/10.1056/NEJMsr2214184?casa_token=Ha74w_FSAZcAAAAA:TDSTolSZsBPiFiSMkgdYELeUZLq8j_4YcblfLQOdJnbg5qLmLs5HktLlrG6800L9PyPje6bXq3KgeA">https://www.nejm.org/doi/pdf/10.1056/NEJMsr2214184?casa_token=Ha74w_FSAZcAAAAA:TDSTolSZsBPiFiSMkgdYELeUZLq8j_4YcblfLQOdJnbg5qLmLs5HktLlrG6800L9PyPje6bXq3KgeA</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Benefits%2C%20Limits%2C%20and%20Risks%20of%20GPT-4%20as%20an%20AI%20Chatbot%20for%20Medicine%20%7C%20NEJM&amp;rft.identifier=https%3A%2F%2Fwww.nejm.org%2Fdoi%2Fpdf%2F10.1056%2FNEJMsr2214184%3Fcasa_token%3DHa74w_FSAZcAAAAA%3ATDSTolSZsBPiFiSMkgdYELeUZLq8j_4YcblfLQOdJnbg5qLmLs5HktLlrG6800L9PyPje6bXq3KgeA"></span>
  <div class="csl-entry">Beutel, Gernot, Eline Geerits, and Jan T. Kielstein. 2023. “Artificial Hallucination: GPT on LSD?” <i>Critical Care</i> 27 (1): 148. <a href="https://doi.org/10.1186/s13054-023-04425-6">https://doi.org/10.1186/s13054-023-04425-6</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1186%2Fs13054-023-04425-6&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Artificial%20hallucination%3A%20GPT%20on%20LSD%3F&amp;rft.jtitle=Critical%20Care&amp;rft.stitle=Crit%20Care&amp;rft.volume=27&amp;rft.issue=1&amp;rft.aufirst=Gernot&amp;rft.aulast=Beutel&amp;rft.au=Gernot%20Beutel&amp;rft.au=Eline%20Geerits&amp;rft.au=Jan%20T.%20Kielstein&amp;rft.date=2023-04-18&amp;rft.pages=148&amp;rft.issn=1364-8535&amp;rft.language=en"></span>
  <div class="csl-entry">Chang, Yupeng, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, et al. 2023. “A Survey on Evaluation of Large Language Models.” arXiv. <a href="http://arxiv.org/abs/2307.03109">http://arxiv.org/abs/2307.03109</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=A%20Survey%20on%20Evaluation%20of%20Large%20Language%20Models&amp;rft.description=Large%20language%20models%20(LLMs)%20are%20gaining%20increasing%20popularity%20in%20both%20academia%20and%20industry%2C%20owing%20to%20their%20unprecedented%20performance%20in%20various%20applications.%20As%20LLMs%20continue%20to%20play%20a%20vital%20role%20in%20both%20research%20and%20daily%20use%2C%20their%20evaluation%20becomes%20increasingly%20critical%2C%20not%20only%20at%20the%20task%20level%2C%20but%20also%20at%20the%20society%20level%20for%20better%20understanding%20of%20their%20potential%20risks.%20Over%20the%20past%20years%2C%20significant%20efforts%20have%20been%20made%20to%20examine%20LLMs%20from%20various%20perspectives.%20This%20paper%20presents%20a%20comprehensive%20review%20of%20these%20evaluation%20methods%20for%20LLMs%2C%20focusing%20on%20three%20key%20dimensions%3A%20what%20to%20evaluate%2C%20where%20to%20evaluate%2C%20and%20how%20to%20evaluate.%20Firstly%2C%20we%20provide%20an%20overview%20from%20the%20perspective%20of%20evaluation%20tasks%2C%20encompassing%20general%20natural%20language%20processing%20tasks%2C%20reasoning%2C%20medical%20usage%2C%20ethics%2C%20educations%2C%20natural%20and%20social%20sciences%2C%20agent%20applications%2C%20and%20other%20areas.%20Secondly%2C%20we%20answer%20the%20%60where'%20and%20%60how'%20questions%20by%20diving%20into%20the%20evaluation%20methods%20and%20benchmarks%2C%20which%20serve%20as%20crucial%20components%20in%20assessing%20performance%20of%20LLMs.%20Then%2C%20we%20summarize%20the%20success%20and%20failure%20cases%20of%20LLMs%20in%20different%20tasks.%20Finally%2C%20we%20shed%20light%20on%20several%20future%20challenges%20that%20lie%20ahead%20in%20LLMs%20evaluation.%20Our%20aim%20is%20to%20offer%20invaluable%20insights%20to%20researchers%20in%20the%20realm%20of%20LLMs%20evaluation%2C%20thereby%20aiding%20the%20development%20of%20more%20proficient%20LLMs.%20Our%20key%20point%20is%20that%20evaluation%20should%20be%20treated%20as%20an%20essential%20discipline%20to%20better%20assist%20the%20development%20of%20LLMs.%20We%20consistently%20maintain%20the%20related%20open-source%20materials%20at%3A%20https%3A%2F%2Fgithub.com%2FMLGroupJLU%2FLLM-eval-survey.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F2307.03109&amp;rft.aufirst=Yupeng&amp;rft.aulast=Chang&amp;rft.au=Yupeng%20Chang&amp;rft.au=Xu%20Wang&amp;rft.au=Jindong%20Wang&amp;rft.au=Yuan%20Wu&amp;rft.au=Linyi%20Yang&amp;rft.au=Kaijie%20Zhu&amp;rft.au=Hao%20Chen&amp;rft.au=Xiaoyuan%20Yi&amp;rft.au=Cunxiang%20Wang&amp;rft.au=Yidong%20Wang&amp;rft.au=Wei%20Ye&amp;rft.au=Yue%20Zhang&amp;rft.au=Yi%20Chang&amp;rft.au=Philip%20S.%20Yu&amp;rft.au=Qiang%20Yang&amp;rft.au=Xing%20Xie&amp;rft.date=2023-08-28"></span>
  <div class="csl-entry">Emsley, Robin. 2023. “ChatGPT: These Are Not Hallucinations – They’re Fabrications and Falsifications.” <i>Schizophrenia</i> 9 (1): 1–2. <a href="https://doi.org/10.1038/s41537-023-00379-4">https://doi.org/10.1038/s41537-023-00379-4</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1038%2Fs41537-023-00379-4&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=ChatGPT%3A%20these%20are%20not%20hallucinations%20%E2%80%93%20they%E2%80%99re%20fabrications%20and%20falsifications&amp;rft.jtitle=Schizophrenia&amp;rft.stitle=Schizophr&amp;rft.volume=9&amp;rft.issue=1&amp;rft.aufirst=Robin&amp;rft.aulast=Emsley&amp;rft.au=Robin%20Emsley&amp;rft.date=2023-08-19&amp;rft.pages=1-2&amp;rft.spage=1&amp;rft.epage=2&amp;rft.issn=2754-6993&amp;rft.language=en"></span>
  <div class="csl-entry">Goddard, Jerome. 2023. “Hallucinations in ChatGPT: A Cautionary Tale for Biomedical Researchers.” <i>The American Journal of Medicine</i> 0 (0). <a href="https://doi.org/10.1016/j.amjmed.2023.06.012">https://doi.org/10.1016/j.amjmed.2023.06.012</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.amjmed.2023.06.012&amp;rft_id=info%3Apmid%2F37369274&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Hallucinations%20in%20ChatGPT%3A%20A%20Cautionary%20Tale%20for%20Biomedical%20Researchers&amp;rft.jtitle=The%20American%20Journal%20of%20Medicine&amp;rft.stitle=The%20American%20Journal%20of%20Medicine&amp;rft.volume=0&amp;rft.issue=0&amp;rft.aufirst=Jerome&amp;rft.aulast=Goddard&amp;rft.au=Jerome%20Goddard&amp;rft.date=2023-06-25&amp;rft.issn=0002-9343%2C%201555-7162&amp;rft.language=English"></span>
  <div class="csl-entry">Hanna, Elias, Alija Levic, Dr Aris Alissandrakis, and Dr Johan Hagelbäck. n.d. “Comparative Analysis of Language Models: Hallucinations in ChatGPT.”</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Comparative%20Analysis%20of%20Language%20Models%3A%20hallucinations%20in%20ChatGPT&amp;rft.aufirst=Elias&amp;rft.aulast=Hanna&amp;rft.au=Elias%20Hanna&amp;rft.au=Alija%20Levic&amp;rft.au=Dr%20Aris%20Alissandrakis&amp;rft.au=Dr%20Johan%20Hagelb%C3%A4ck&amp;rft.language=en"></span>
  <div class="csl-entry">Hanneke, Steve, Adam Tauman Kalai, Gautam Kamath, and Christos Tzamos. 2018. “Actively Avoiding Nonsense in Generative Models.” In <i>Proceedings of the 31st&nbsp; Conference On Learning Theory</i>, 209–27. PMLR. <a href="https://proceedings.mlr.press/v75/hanneke18a.html">https://proceedings.mlr.press/v75/hanneke18a.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Actively%20Avoiding%20Nonsense%20in%20Generative%20Models&amp;rft.btitle=Proceedings%20of%20the%2031st%20%20Conference%20On%20Learning%20Theory&amp;rft.publisher=PMLR&amp;rft.aufirst=Steve&amp;rft.aulast=Hanneke&amp;rft.au=Steve%20Hanneke&amp;rft.au=Adam%20Tauman%20Kalai&amp;rft.au=Gautam%20Kamath&amp;rft.au=Christos%20Tzamos&amp;rft.date=2018-07-03&amp;rft.pages=209-227&amp;rft.spage=209&amp;rft.epage=227&amp;rft.language=en"></span>
  <div class="csl-entry">Ji, Ziwei, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, et al. 2023. “Survey of Hallucination in Natural Language Generation.” <i>ACM Computing Surveys</i> 55 (12): 1–38. <a href="https://doi.org/10.1145/3571730">https://doi.org/10.1145/3571730</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F3571730&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Survey%20of%20Hallucination%20in%20Natural%20Language%20Generation&amp;rft.jtitle=ACM%20Computing%20Surveys&amp;rft.stitle=ACM%20Comput.%20Surv.&amp;rft.volume=55&amp;rft.issue=12&amp;rft.aufirst=Ziwei&amp;rft.aulast=Ji&amp;rft.au=Ziwei%20Ji&amp;rft.au=Nayeon%20Lee&amp;rft.au=Rita%20Frieske&amp;rft.au=Tiezheng%20Yu&amp;rft.au=Dan%20Su&amp;rft.au=Yan%20Xu&amp;rft.au=Etsuko%20Ishii&amp;rft.au=Yejin%20Bang&amp;rft.au=Wenliang%20Dai&amp;rft.au=Andrea%20Madotto&amp;rft.au=Pascale%20Fung&amp;rft.date=2023-12-31&amp;rft.pages=1-38&amp;rft.spage=1&amp;rft.epage=38&amp;rft.issn=0360-0300%2C%201557-7341"></span>
  <div class="csl-entry">Li, Junyi, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. “HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.” arXiv. <a href="http://arxiv.org/abs/2305.11747">http://arxiv.org/abs/2305.11747</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=HaluEval%3A%20A%20Large-Scale%20Hallucination%20Evaluation%20Benchmark%20for%20Large%20Language%20Models&amp;rft.description=Large%20language%20models%20(LLMs)%2C%20such%20as%20ChatGPT%2C%20are%20prone%20to%20generate%20hallucinations%2C%20%5Cie%20content%20that%20conflicts%20with%20the%20source%20or%20cannot%20be%20verified%20by%20the%20factual%20knowledge.%20To%20understand%20what%20types%20of%20content%20and%20to%20which%20extent%20LLMs%20are%20apt%20to%20hallucinate%2C%20we%20introduce%20the%20Hallucination%20Evaluation%20for%20Large%20Language%20Models%20(HaluEval)%20benchmark%2C%20a%20large%20collection%20of%20generated%20and%20human-annotated%20hallucinated%20samples%20for%20evaluating%20the%20performance%20of%20LLMs%20in%20recognizing%20hallucination.%20To%20generate%20these%20samples%2C%20we%20propose%20a%20ChatGPT-based%20two-step%20framework%2C%20%5Cie%20sampling-then-filtering.%20Besides%2C%20we%20also%20hire%20some%20human%20labelers%20to%20annotate%20the%20hallucinations%20in%20ChatGPT%20responses.%20The%20empirical%20results%20suggest%20that%20ChatGPT%20is%20likely%20to%20generate%20hallucinated%20content%20in%20specific%20topics%20by%20fabricating%20unverifiable%20information%20(%5Cie%20about%20%2411.4%5C%25%24%20user%20queries).%20Moreover%2C%20existing%20LLMs%20face%20great%20challenges%20in%20recognizing%20the%20hallucinations%20in%20texts.%20While%2C%20our%20experiments%20also%20prove%20that%20the%20hallucination%20recognition%20can%20be%20improved%20by%20providing%20external%20knowledge%20or%20adding%20reasoning%20steps.%20Our%20benchmark%20can%20be%20accessed%20at%20https%3A%2F%2Fgithub.com%2FRUCAIBox%2FHaluEval.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F2305.11747&amp;rft.aufirst=Junyi&amp;rft.aulast=Li&amp;rft.au=Junyi%20Li&amp;rft.au=Xiaoxue%20Cheng&amp;rft.au=Wayne%20Xin%20Zhao&amp;rft.au=Jian-Yun%20Nie&amp;rft.au=Ji-Rong%20Wen&amp;rft.date=2023-05-22"></span>
  <div class="csl-entry">Li, Zihao. 2023. “The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination.” arXiv. <a href="https://doi.org/10.48550/arXiv.2304.14347">https://doi.org/10.48550/arXiv.2304.14347</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2304.14347&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=The%20Dark%20Side%20of%20ChatGPT%3A%20Legal%20and%20Ethical%20Challenges%20from%20Stochastic%20Parrots%20and%20Hallucination&amp;rft.description=With%20the%20launch%20of%20ChatGPT%2C%20Large%20Language%20Models%20(LLMs)%20are%20shaking%20up%20our%20whole%20society%2C%20rapidly%20altering%20the%20way%20we%20think%2C%20create%20and%20live.%20For%20instance%2C%20the%20GPT%20integration%20in%20Bing%20has%20altered%20our%20approach%20to%20online%20searching.%20While%20nascent%20LLMs%20have%20many%20advantages%2C%20new%20legal%20and%20ethical%20risks%20are%20also%20emerging%2C%20stemming%20in%20particular%20from%20stochastic%20parrots%20and%20hallucination.%20The%20EU%20is%20the%20first%20and%20foremost%20jurisdiction%20that%20has%20focused%20on%20the%20regulation%20of%20AI%20models.%20However%2C%20the%20risks%20posed%20by%20the%20new%20LLMs%20are%20likely%20to%20be%20underestimated%20by%20the%20emerging%20EU%20regulatory%20paradigm.%20Therefore%2C%20this%20correspondence%20warns%20that%20the%20European%20AI%20regulatory%20paradigm%20must%20evolve%20further%20to%20mitigate%20such%20risks.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2304.14347&amp;rft.aufirst=Zihao&amp;rft.aulast=Li&amp;rft.au=Zihao%20Li&amp;rft.date=2023-04-21"></span>
  <div class="csl-entry">Liang, Percy, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, et al. 2022. “Holistic Evaluation of Language Models.” arXiv. <a href="https://doi.org/10.48550/arXiv.2211.09110">https://doi.org/10.48550/arXiv.2211.09110</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2211.09110&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Holistic%20Evaluation%20of%20Language%20Models&amp;rft.description=Language%20models%20(LMs)%20are%20becoming%20the%20foundation%20for%20almost%20all%20major%20language%20technologies%2C%20but%20their%20capabilities%2C%20limitations%2C%20and%20risks%20are%20not%20well%20understood.%20We%20present%20Holistic%20Evaluation%20of%20Language%20Models%20(HELM)%20to%20improve%20the%20transparency%20of%20language%20models.%20First%2C%20we%20taxonomize%20the%20vast%20space%20of%20potential%20scenarios%20(i.e.%20use%20cases)%20and%20metrics%20(i.e.%20desiderata)%20that%20are%20of%20interest%20for%20LMs.%20Then%20we%20select%20a%20broad%20subset%20based%20on%20coverage%20and%20feasibility%2C%20noting%20what's%20missing%20or%20underrepresented%20(e.g.%20question%20answering%20for%20neglected%20English%20dialects%2C%20metrics%20for%20trustworthiness).%20Second%2C%20we%20adopt%20a%20multi-metric%20approach%3A%20We%20measure%207%20metrics%20(accuracy%2C%20calibration%2C%20robustness%2C%20fairness%2C%20bias%2C%20toxicity%2C%20and%20efficiency)%20for%20each%20of%2016%20core%20scenarios%20when%20possible%20(87.5%25%20of%20the%20time).%20This%20ensures%20metrics%20beyond%20accuracy%20don't%20fall%20to%20the%20wayside%2C%20and%20that%20trade-offs%20are%20clearly%20exposed.%20We%20also%20perform%207%20targeted%20evaluations%2C%20based%20on%2026%20targeted%20scenarios%2C%20to%20analyze%20specific%20aspects%20(e.g.%20reasoning%2C%20disinformation).%20Third%2C%20we%20conduct%20a%20large-scale%20evaluation%20of%2030%20prominent%20language%20models%20(spanning%20open%2C%20limited-access%2C%20and%20closed%20models)%20on%20all%2042%20scenarios%2C%2021%20of%20which%20were%20not%20previously%20used%20in%20mainstream%20LM%20evaluation.%20Prior%20to%20HELM%2C%20models%20on%20average%20were%20evaluated%20on%20just%2017.9%25%20of%20the%20core%20HELM%20scenarios%2C%20with%20some%20prominent%20models%20not%20sharing%20a%20single%20scenario%20in%20common.%20We%20improve%20this%20to%2096.0%25%3A%20now%20all%2030%20models%20have%20been%20densely%20benchmarked%20on%20the%20same%20core%20scenarios%20and%20metrics%20under%20standardized%20conditions.%20Our%20evaluation%20surfaces%2025%20top-level%20findings.%20For%20full%20transparency%2C%20we%20release%20all%20raw%20model%20prompts%20and%20completions%20publicly%20for%20further%20analysis%2C%20as%20well%20as%20a%20general%20modular%20toolkit.%20We%20intend%20for%20HELM%20to%20be%20a%20living%20benchmark%20for%20the%20community%2C%20continuously%20updated%20with%20new%20scenarios%2C%20metrics%2C%20and%20models.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2211.09110&amp;rft.aufirst=Percy&amp;rft.aulast=Liang&amp;rft.au=Percy%20Liang&amp;rft.au=Rishi%20Bommasani&amp;rft.au=Tony%20Lee&amp;rft.au=Dimitris%20Tsipras&amp;rft.au=Dilara%20Soylu&amp;rft.au=Michihiro%20Yasunaga&amp;rft.au=Yian%20Zhang&amp;rft.au=Deepak%20Narayanan&amp;rft.au=Yuhuai%20Wu&amp;rft.au=Ananya%20Kumar&amp;rft.au=Benjamin%20Newman&amp;rft.au=Binhang%20Yuan&amp;rft.au=Bobby%20Yan&amp;rft.au=Ce%20Zhang&amp;rft.au=Christian%20Cosgrove&amp;rft.au=Christopher%20D.%20Manning&amp;rft.au=Christopher%20R%C3%A9&amp;rft.au=Diana%20Acosta-Navas&amp;rft.au=Drew%20A.%20Hudson&amp;rft.au=Eric%20Zelikman&amp;rft.au=Esin%20Durmus&amp;rft.au=Faisal%20Ladhak&amp;rft.au=Frieda%20Rong&amp;rft.au=Hongyu%20Ren&amp;rft.au=Huaxiu%20Yao&amp;rft.au=Jue%20Wang&amp;rft.au=Keshav%20Santhanam&amp;rft.au=Laurel%20Orr&amp;rft.au=Lucia%20Zheng&amp;rft.au=Mert%20Yuksekgonul&amp;rft.au=Mirac%20Suzgun&amp;rft.au=Nathan%20Kim&amp;rft.au=Neel%20Guha&amp;rft.au=Niladri%20Chatterji&amp;rft.au=Omar%20Khattab&amp;rft.au=Peter%20Henderson&amp;rft.au=Qian%20Huang&amp;rft.au=Ryan%20Chi&amp;rft.au=Sang%20Michael%20Xie&amp;rft.au=Shibani%20Santurkar&amp;rft.au=Surya%20Ganguli&amp;rft.au=Tatsunori%20Hashimoto&amp;rft.au=Thomas%20Icard&amp;rft.au=Tianyi%20Zhang&amp;rft.au=Vishrav%20Chaudhary&amp;rft.au=William%20Wang&amp;rft.au=Xuechen%20Li&amp;rft.au=Yifan%20Mai&amp;rft.au=Yuhui%20Zhang&amp;rft.au=Yuta%20Koreeda&amp;rft.date=2022-11-16"></span>
  <div class="csl-entry">MacKinnon, Neil J., Vanessa Emery, Jennifer Waller, Brittany Ange, Preshit Ambade, Munira Gunja, and Emma Watson. 2023. “Mapping Health Disparities in 11 High-Income Nations.” <i>JAMA Network Open</i> 6 (7): e2322310. <a href="https://doi.org/10.1001/jamanetworkopen.2023.22310">https://doi.org/10.1001/jamanetworkopen.2023.22310</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1001%2Fjamanetworkopen.2023.22310&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Mapping%20Health%20Disparities%20in%2011%20High-Income%20Nations&amp;rft.jtitle=JAMA%20Network%20Open&amp;rft.stitle=JAMA%20Network%20Open&amp;rft.volume=6&amp;rft.issue=7&amp;rft.aufirst=Neil%20J.&amp;rft.aulast=MacKinnon&amp;rft.au=Neil%20J.%20MacKinnon&amp;rft.au=Vanessa%20Emery&amp;rft.au=Jennifer%20Waller&amp;rft.au=Brittany%20Ange&amp;rft.au=Preshit%20Ambade&amp;rft.au=Munira%20Gunja&amp;rft.au=Emma%20Watson&amp;rft.date=2023-07-07&amp;rft.pages=e2322310&amp;rft.issn=2574-3805"></span>
  <div class="csl-entry">“Mathematics | Free Full-Text | A Mathematical Investigation of Hallucination and Creativity in GPT Models.” n.d. Accessed September 12, 2023. <a href="https://www.mdpi.com/2227-7390/11/10/2320">https://www.mdpi.com/2227-7390/11/10/2320</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Mathematics%20%7C%20Free%20Full-Text%20%7C%20A%20Mathematical%20Investigation%20of%20Hallucination%20and%20Creativity%20in%20GPT%20Models&amp;rft.identifier=https%3A%2F%2Fwww.mdpi.com%2F2227-7390%2F11%2F10%2F2320"></span>
  <div class="csl-entry">Salvagno, Michele, Fabio Silvio Taccone, and Alberto Giovanni Gerli. 2023. “Artificial Intelligence Hallucinations.” <i>Critical Care</i> 27 (1): 180. <a href="https://doi.org/10.1186/s13054-023-04473-y">https://doi.org/10.1186/s13054-023-04473-y</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1186%2Fs13054-023-04473-y&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Artificial%20intelligence%20hallucinations&amp;rft.jtitle=Critical%20Care&amp;rft.stitle=Critical%20Care&amp;rft.volume=27&amp;rft.issue=1&amp;rft.aufirst=Michele&amp;rft.aulast=Salvagno&amp;rft.au=Michele%20Salvagno&amp;rft.au=Fabio%20Silvio%20Taccone&amp;rft.au=Alberto%20Giovanni%20Gerli&amp;rft.date=2023-05-10&amp;rft.pages=180&amp;rft.issn=1364-8535"></span>
  <div class="csl-entry">Xu, Weijia, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat. 2023. “Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection.” <i>Transactions of the Association for Computational Linguistics</i> 11 (June): 546–64. <a href="https://doi.org/10.1162/tacl_a_00563">https://doi.org/10.1162/tacl_a_00563</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1162%2Ftacl_a_00563&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Understanding%20and%20Detecting%20Hallucinations%20in%20Neural%20Machine%20Translation%20via%20Model%20Introspection&amp;rft.jtitle=Transactions%20of%20the%20Association%20for%20Computational%20Linguistics&amp;rft.stitle=Transactions%20of%20the%20Association%20for%20Computational%20Linguistics&amp;rft.volume=11&amp;rft.aufirst=Weijia&amp;rft.aulast=Xu&amp;rft.au=Weijia%20Xu&amp;rft.au=Sweta%20Agrawal&amp;rft.au=Eleftheria%20Briakou&amp;rft.au=Marianna%20J.%20Martindale&amp;rft.au=Marine%20Carpuat&amp;rft.date=2023-06-12&amp;rft.pages=546-564&amp;rft.spage=546&amp;rft.epage=564&amp;rft.issn=2307-387X"></span>
  <div class="csl-entry">Zha, Yuheng, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. “AlignScore: Evaluating Factual Consistency with a Unified Alignment Function.” arXiv. <a href="https://doi.org/10.48550/arXiv.2305.16739">https://doi.org/10.48550/arXiv.2305.16739</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2305.16739&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=AlignScore%3A%20Evaluating%20Factual%20Consistency%20with%20a%20Unified%20Alignment%20Function&amp;rft.description=Many%20text%20generation%20applications%20require%20the%20generated%20text%20to%20be%20factually%20consistent%20with%20input%20information.%20Automatic%20evaluation%20of%20factual%20consistency%20is%20challenging.%20Previous%20work%20has%20developed%20various%20metrics%20that%20often%20depend%20on%20specific%20functions%2C%20such%20as%20natural%20language%20inference%20(NLI)%20or%20question%20answering%20(QA)%2C%20trained%20on%20limited%20data.%20Those%20metrics%20thus%20can%20hardly%20assess%20diverse%20factual%20inconsistencies%20(e.g.%2C%20contradictions%2C%20hallucinations)%20that%20occur%20in%20varying%20inputs%2Foutputs%20(e.g.%2C%20sentences%2C%20documents)%20from%20different%20tasks.%20In%20this%20paper%2C%20we%20propose%20AlignScore%2C%20a%20new%20holistic%20metric%20that%20applies%20to%20a%20variety%20of%20factual%20inconsistency%20scenarios%20as%20above.%20AlignScore%20is%20based%20on%20a%20general%20function%20of%20information%20alignment%20between%20two%20arbitrary%20text%20pieces.%20Crucially%2C%20we%20develop%20a%20unified%20training%20framework%20of%20the%20alignment%20function%20by%20integrating%20a%20large%20diversity%20of%20data%20sources%2C%20resulting%20in%204.7M%20training%20examples%20from%207%20well-established%20tasks%20(NLI%2C%20QA%2C%20paraphrasing%2C%20fact%20verification%2C%20information%20retrieval%2C%20semantic%20similarity%2C%20and%20summarization).%20We%20conduct%20extensive%20experiments%20on%20large-scale%20benchmarks%20including%2022%20evaluation%20datasets%2C%20where%2019%20of%20the%20datasets%20were%20never%20seen%20in%20the%20alignment%20training.%20AlignScore%20achieves%20substantial%20improvement%20over%20a%20wide%20range%20of%20previous%20metrics.%20Moreover%2C%20AlignScore%20(355M%20parameters)%20matches%20or%20even%20outperforms%20metrics%20based%20on%20ChatGPT%20and%20GPT-4%20that%20are%20orders%20of%20magnitude%20larger.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2305.16739&amp;rft.aufirst=Yuheng&amp;rft.aulast=Zha&amp;rft.au=Yuheng%20Zha&amp;rft.au=Yichi%20Yang&amp;rft.au=Ruichen%20Li&amp;rft.au=Zhiting%20Hu&amp;rft.date=2023-05-26"></span>
  <div class="csl-entry">Zhang, Muru, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. 2023. “How Language Model Hallucinations Can Snowball.” arXiv. <a href="https://doi.org/10.48550/arXiv.2305.13534">https://doi.org/10.48550/arXiv.2305.13534</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2305.13534&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=How%20Language%20Model%20Hallucinations%20Can%20Snowball&amp;rft.description=A%20major%20risk%20of%20using%20language%20models%20in%20practical%20applications%20is%20their%20tendency%20to%20hallucinate%20incorrect%20statements.%20Hallucinations%20are%20often%20attributed%20to%20knowledge%20gaps%20in%20LMs%2C%20but%20we%20hypothesize%20that%20in%20some%20cases%2C%20when%20justifying%20previously%20generated%20hallucinations%2C%20LMs%20output%20false%20claims%20that%20they%20can%20separately%20recognize%20as%20incorrect.%20We%20construct%20three%20question-answering%20datasets%20where%20ChatGPT%20and%20GPT-4%20often%20state%20an%20incorrect%20answer%20and%20offer%20an%20explanation%20with%20at%20least%20one%20incorrect%20claim.%20Crucially%2C%20we%20find%20that%20ChatGPT%20and%20GPT-4%20can%20identify%2067%25%20and%2087%25%20of%20their%20own%20mistakes%2C%20respectively.%20We%20refer%20to%20this%20phenomenon%20as%20hallucination%20snowballing%3A%20an%20LM%20over-commits%20to%20early%20mistakes%2C%20leading%20to%20more%20mistakes%20that%20it%20otherwise%20would%20not%20make.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2305.13534&amp;rft.aufirst=Muru&amp;rft.aulast=Zhang&amp;rft.au=Muru%20Zhang&amp;rft.au=Ofir%20Press&amp;rft.au=William%20Merrill&amp;rft.au=Alisa%20Liu&amp;rft.au=Noah%20A.%20Smith&amp;rft.date=2023-05-22"></span>
</div>
</body>
</html>
